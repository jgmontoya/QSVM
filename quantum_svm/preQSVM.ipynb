{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script's used as an initial search for a promising configuration. It uses smaller sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.aqua.utils import split_dataset_to_data_and_labels, map_label_to_class_name\n",
    "from qiskit.aqua.input import ClassificationInput\n",
    "from qiskit.aqua import run_algorithm, QuantumInstance\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Real Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up account \n",
    "from qiskit import IBMQ\n",
    "account_token = ''\n",
    "login_address = 'https://api.quantum-computing.ibm.com/api/Hubs/ibm-q/Groups/open/Projects/main'\n",
    "#IBMQ.enable_account(account_token, login_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# this is the iris loading part edited from the tutorial to return correct data structure. \n",
    "# The part reducing dimension with PCA is commented out. \n",
    "# Creation of test set is modified to allow test set size larger than 1. \n",
    "# train_size: integer, train set size per type of sample\n",
    "# test_size: integer, test set size per type of sample\n",
    "# If there's not enough sample, this method will return all available samples in the dataset.\n",
    "# all data are standardized for gaussian around 0 with unit variance and scaled to range (-1, +1).\n",
    "\n",
    "def Iris(train_size, test_size, PLOT_DATA=True, random_seed=666):\n",
    "    if train_size > 150 or test_size > 150:\n",
    "        raise Exception('Training or testing set shouldn\\'t be larger than the size of IRIS, which is 150.')\n",
    "    \n",
    "    class_labels = [r'A', r'B', r'C']\n",
    "    data, target = load_iris(True)\n",
    "    test_ratio = float(test_size)/float(train_size + test_size)\n",
    "    sample_train, sample_test, label_train, label_test = train_test_split(data, target, test_size=test_ratio, random_state=random_seed)\n",
    "\n",
    "    # Now reduce number of features to number of qubits\n",
    "    # pca = PCA(n_components=n).fit(sample_train)\n",
    "    # sample_train = pca.transform(sample_train)\n",
    "    # sample_test = pca.transform(sample_test)\n",
    "\n",
    "    \n",
    "    # Now we standarize for gaussian around 0 with unit variance\n",
    "    std_scale = StandardScaler().fit(sample_train)\n",
    "    sample_train = std_scale.transform(sample_train)\n",
    "    sample_test = std_scale.transform(sample_test)\n",
    "    \n",
    "    # Scale to the range (-1,+1)\n",
    "    samples = np.append(sample_train, sample_test, axis=0)\n",
    "    minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "    sample_train = minmax_scale.transform(sample_train)\n",
    "    sample_test = minmax_scale.transform(sample_test)\n",
    "\n",
    "    # Pick training size number of samples from each distro\n",
    "    train_set = {key: (sample_train[label_train == k, :])[:train_size] for k, key in enumerate(class_labels)}\n",
    "    test_set = {key: (sample_test[label_test == k, :])[:test_size] for k, key in enumerate(class_labels)}\n",
    "\n",
    "    if PLOT_DATA:\n",
    "        for k in range(0, 3):\n",
    "            plt.scatter(sample_train[label_train == k, 0][:train_size],\n",
    "                        sample_train[label_train == k, 1][:train_size])\n",
    "\n",
    "        plt.title(\"train set\")\n",
    "        plt.show()\n",
    "        \n",
    "        for k in range(0, 3):\n",
    "            plt.scatter(sample_test[label_test == k, 0][:test_size],\n",
    "                        sample_test[label_test == k, 1][:test_size])\n",
    "\n",
    "        plt.title(\"test set\")\n",
    "        plt.show()\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.aqua.components.feature_maps import SecondOrderExpansion, FirstOrderExpansion, PauliExpansion, self_product\n",
    "#from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "\n",
    "feature_dim = 4 # Iris contains 4 features: sepal length, sepal width, petal length, petal width\n",
    "feature_map_depth = 2 # circuit depth of feature map\n",
    "\n",
    "\n",
    "\n",
    "# Making a list of feature maps to test in combination with all multiclass extensions\n",
    "\n",
    "\n",
    "# Creating train and test sets from Iris dataset\n",
    "train_set, test_set = Iris(20,5)\n",
    "extra_train_set, extra_test_set = Iris(1,5)\n",
    "datapoints, class_to_label = split_dataset_to_data_and_labels(extra_test_set)\n",
    "\n",
    "\n",
    "# This part converts data from Iris method to the format required by QSVM.\n",
    "temp = [test_set[k] for k in test_set]\n",
    "total_array = np.concatenate(temp)\n",
    "algo_input = ClassificationInput(train_set, test_set, total_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method runs aqua with all possible combination of feature maps and multi-class expansions from a given config\n",
    "def run_aqua(algo_input, real_backend = False, nshots = 1024):\n",
    "    # This json object sets the basic config for the QSVM algorithm object. \n",
    "    # Since programmatic object themselves are not yet fully implemented in Aqua, \n",
    "    # this is the only possible way to define algorithm objects.\n",
    "    \n",
    "    #TODO: parameters for feature maps\n",
    "    #TODO: \n",
    "    aqua_dict = {\n",
    "        'problem': {'name': 'classification'},\n",
    "        'algorithm': {'name': 'QSVM'},\n",
    "        'backend': {},\n",
    "        'multiclass_extension': {},\n",
    "        'feature_map': {}\n",
    "    }\n",
    "    \n",
    "    # This part defines backend\n",
    "    if real_backend:\n",
    "        aqua_dict['backend'] = {'provider': 'qiskit.IBMQ', 'name': 'ibmq_16_melbourne', 'shots': nshots}\n",
    "    else:\n",
    "        aqua_dict['backend'] =  {'provider': 'qiskit.BasicAer', 'name': 'qasm_simulator', 'shots': nshots}\n",
    "        \n",
    "    # list of all extensions and feature maps.\n",
    "    extensions = [\n",
    "       {'name': 'OneAgainstRest'},\n",
    "       {'name': 'AllPairs'}, \n",
    "       {'name': 'ErrorCorrectingCode', 'code_size': 5}]\n",
    "\n",
    "    feature_maps = [\n",
    "       {'name': 'PauliExpansion', 'depth': 2, 'paulis':['Z', 'ZZ'], 'entanglement': 'full'},\n",
    "       {'name': 'RawFeatureVector', 'feature_dimension': 4},\n",
    "       {'name': 'PauliZExpansion', 'depth': 2, 'z_order': 2, 'entanglement': 'full'},\n",
    "       {'name': 'SecondOrderExpansion', 'depth': 2, 'entanglement': 'full'},\n",
    "       # Linear entanglement means nearest neighbour entanglement\n",
    "       {'name': 'FirstOrderExpansion', 'depth': 2}]\n",
    "    \n",
    "    # looping over them\n",
    "    for extension in extensions:\n",
    "        for feature_map in feature_maps:\n",
    "            aqua_dict['multiclass_extension'] = extension\n",
    "            aqua_dict['feature_map'] = feature_map\n",
    "            result = run_algorithm(aqua_dict, algo_input)\n",
    "            print(\"\\n----- Using multiclass extension: '{}' -----\\n\".format(extension['name']))\n",
    "            print(\"\\n----- Using feature map: '{}' -----\\n\".format(feature_map['name']))\n",
    "            for k,v in result.items():\n",
    "                print(\"'{}' : {}\".format(k, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_aqua(algo_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
